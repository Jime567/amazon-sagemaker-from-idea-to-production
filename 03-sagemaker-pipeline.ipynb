{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575c15ae-ffe0-4d93-84d8-bf96278c0372",
   "metadata": {},
   "source": [
    "# Step 3: Add an ML pipeline\n",
    "\n",
    "In this step we automate our end-to-end ML workflow using [Amazon SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines/) and [Amazon SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html). We make feature engineering re-usable, repeatable, and scaleable using [Amazon SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/).\n",
    "\n",
    "![](img/six-steps-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca42d3f-7231-44cc-83f4-a54c6a7995f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import io\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput, \n",
    "    ScriptProcessor\n",
    ")\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep, \n",
    "    TrainingStep, \n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger, \n",
    "    ParameterFloat, \n",
    "    ParameterString, \n",
    "    ParameterBoolean\n",
    ")\n",
    "from sagemaker.workflow.clarify_check_step import (\n",
    "    ModelBiasCheckConfig, \n",
    "    ClarifyCheckStep, \n",
    "    ModelExplainabilityCheckConfig\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource, \n",
    "    ModelMetrics, \n",
    "    FileSource\n",
    ")\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "\n",
    "from sagemaker.image_uris import retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ac5a8-2730-45fd-8bed-1559c422748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r \n",
    "\n",
    "%store\n",
    "\n",
    "try:\n",
    "    initialized\n",
    "except NameError:\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] YOU HAVE TO RUN 00-start-here notebook   \")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd6d8b-a230-4bf8-a575-2c6dcff55615",
   "metadata": {},
   "source": [
    "## Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b507e-f512-49c9-9f59-6477133bd118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set names of pipeline objects\n",
    "project = \"from-idea-to-prod\"\n",
    "\n",
    "pipeline_name = f\"{project}-pipeline\"\n",
    "pipeline_model_name = f\"{project}-model-xgb\"\n",
    "model_package_group_name = f\"{project}-model-group\"\n",
    "endpoint_config_name = f\"{project}-endpoint-config\"\n",
    "endpoint_name = f\"{project}-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947da3b-e046-4dea-9897-1717c5bbc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set instance types and counts\n",
    "process_instance_type = \"ml.c5.xlarge\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9be066-8452-4058-b50e-e3027cb466b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set S3 urls for processed data\n",
    "train_s3_url = f\"s3://{bucket_name}/{bucket_prefix}/train\"\n",
    "validation_s3_url = f\"s3://{bucket_name}/{bucket_prefix}/validation\"\n",
    "test_s3_url = f\"s3://{bucket_name}/{bucket_prefix}/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d649cf-b64c-4978-b3f5-cf767b7f1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store model_package_group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ea883-3bd9-4947-a0c8-7c3a80c1bbca",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00ef9a-a6fe-4415-9847-bcfa97f32c44",
   "metadata": {},
   "source": [
    "### Setup pipeline parameters\n",
    "SageMaker Pipelines supports [parameterization](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html), which allows you to specify input parameters at runtime without changing your pipeline code. You can use the parameter classes available under the [`sagemaker.workflow.parameters`](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#parameters) module.\n",
    "Parameters have a default value, which you can override by specifying parameter values when starting a pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f370b6-1ee3-4bc6-ae85-5eef37878196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set processing instance type\n",
    "process_instance_type_param = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=process_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance type\n",
    "train_instance_type_param = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=train_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance count\n",
    "train_instance_count_param = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value=train_instance_count\n",
    ")\n",
    "\n",
    "# Set model approval param\n",
    "model_approval_status_param = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "# Set S3 url for input dataset\n",
    "input_s3_url_param = ParameterString(\n",
    "    name=\"InputDataUrl\",\n",
    "    default_value=input_s3_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c3182-d926-4aa5-b8c0-23d7c782f116",
   "metadata": {},
   "source": [
    "### Build the pipeline steps\n",
    "We include the following steps in the pipeline:\n",
    "1. **Data processing step**: runs a SageMaker processing job for feature engineering and dataset split\n",
    "2. **Training step**: runs a SageMaker training job using XGBoost algorithm\n",
    "3. **Register step**: registers a version of the model in the SageMaker model registry\n",
    "\n",
    "We re-use the code from the step 2 notebook and just wrap it in the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe61a9b-ea0e-45b7-b37f-37684c2d7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_session = PipelineSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10b118-2985-41a9-b1af-da7111060f68",
   "metadata": {},
   "source": [
    "#### Processing step\n",
    "We re-use the processor definition and `sklearn_processor.run()` code from the step 2 to create a pipeline processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664feff0-e1e6-4b4e-bcea-d31617725bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=\"0.23-1\",\n",
    "        role=sm_role,\n",
    "        instance_type=process_instance_type_param,\n",
    "        instance_count=1,\n",
    "        sagemaker_session=pipeline_session,\n",
    "        base_job_name=f\"{pipeline_name}/preprocess\",\n",
    "    )\n",
    "    \n",
    "# Define processing step\n",
    "step_process = ProcessingStep(\n",
    "    name=\"PreprocessData\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_s3_url_param, destination=\"/opt/ml/processing/input\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/output/train\", \n",
    "                         destination=train_s3_url),\n",
    "        ProcessingOutput(output_name=\"validation_data\", source=\"/opt/ml/processing/output/validation\",\n",
    "                         destination=validation_s3_url),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/output/test\",\n",
    "                         destination=test_s3_url),\n",
    "    ],\n",
    "    code=\"preprocessing.py\",\n",
    "#    job_arguments=[\"--arg1\", \"arg1_value\", \"--arg2\", \"arg2_value\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21faa341-c920-47fc-bf79-1453f7982f95",
   "metadata": {},
   "source": [
    "#### Training step\n",
    "We re-use the estimator definition and hyperparameters setup code from the step 2 to create a pipeline model training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f8317-4758-420a-b8e9-1a3e23ed3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve(\"xgboost\", region=region, version=\"latest\")\n",
    "\n",
    "# Instantiate an XGBoost estimator object\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=training_image,\n",
    "    role=sm_role, \n",
    "    instance_type=train_instance_type_param,\n",
    "    instance_count=train_instance_count_param,\n",
    "    output_path=output_s3_url,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    base_job_name=f\"{pipeline_name}/train\",\n",
    ")\n",
    "\n",
    "# Define its hyperparameters\n",
    "estimator.set_hyperparameters(\n",
    "    num_round=150, # the number of rounds to run the training\n",
    "    max_depth=5, # maximum depth of a tree\n",
    "    eta=0.5, # step size shrinkage used in updates to prevent overfitting\n",
    "    alpha=2.5, # L1 regularization term on weights\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\", # evaluation metrics for validation data\n",
    "    subsample=0.8, # subsample ratio of the training instance\n",
    "    colsample_bytree=0.8, # subsample ratio of columns when constructing each tree\n",
    "    min_child_weight=3, # minimum sum of instance weight (hessian) needed in a child\n",
    "    early_stopping_rounds=10, # the model trains until the validation score stops improving\n",
    "    verbosity=1, # verbosity of printing messages\n",
    ")\n",
    "\n",
    "# Define training step\n",
    "step_train = TrainingStep(\n",
    "    name=\"Train\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train_data\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation_data\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a5348-9c06-416a-a4b2-0c83e4ea34f4",
   "metadata": {},
   "source": [
    "#### Register step\n",
    "The register step registers a new version of a trained model in the SageMaker Model Registry within a [model package group](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-model-group.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f94ac1-5b7b-41ea-bfef-d836efb8b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_register = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status_param,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79095789-64d7-47b5-952c-d70b8500ed0c",
   "metadata": {},
   "source": [
    "#### Construct the pipeline \n",
    "We don't need to manually define a sequence of the steps, as SageMaker automatically derives the processing flow based on data dependencies between pipeline's steps. You also don't need to manage transfer of artifacts and datasets from one pipeline's step to another, because SageMaker automatically takes care of the data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e7177-84bf-4271-9036-7af3d640daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        process_instance_type_param,\n",
    "        train_instance_type_param,\n",
    "        train_instance_count_param,\n",
    "        model_approval_status_param,\n",
    "        input_s3_url_param,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_register],\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b8030-aea8-45bb-a4b1-f20b09df7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new or update existing Pipeline\n",
    "pipeline.upsert(role_arn=sm_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d05813-b615-463b-9b90-67f45c9444cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_definition = json.loads(pipeline.describe()['PipelineDefinition'])\n",
    "pipeline_definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297037d-a04c-463a-9353-c362fcb3677d",
   "metadata": {},
   "source": [
    "Look at and understand the pipeline definition JSON. For example, you can see, how the pipeline paramemters are defined and how are they used.\n",
    "\n",
    "Definition:\n",
    "```json\n",
    "'Parameters': [{'Name': 'ProcessingInstanceType',\n",
    "   'Type': 'String',\n",
    "   'DefaultValue': 'ml.c5.xlarge'},\n",
    "               ...\n",
    "               ]\n",
    "```\n",
    "\n",
    "Parameter substitution:\n",
    "```json\n",
    "'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType':{'Get':'Parameters.ProcessingInstanceType'}\n",
    "```\n",
    "\n",
    "You can also see that the processing script `preprocessing.py` is automatically uploaded to an Amazon S3 bucket and the S3 url is specified as one of the step's `ProcessingInputs`:\n",
    "```json\n",
    "{'InputName': 'code',\n",
    "      'AppManaged': False,\n",
    "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-XXXX/PreprocessData-e5d4c2b08e616c264c9dc5053871519a/input/code/preprocessing.py',\n",
    "       'LocalPath': '/opt/ml/processing/input/code',\n",
    "       'S3DataType': 'S3Prefix',\n",
    "       'S3InputMode': 'File',\n",
    "       'S3DataDistributionType': 'FullyReplicated',\n",
    "       'S3CompressionType': 'None'}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42089e-1ccf-491c-85f1-4c685f5fae07",
   "metadata": {},
   "source": [
    "## Execute the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30569d9c-08b7-4f86-ad86-02771ce47ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        ProcessingInstanceType=process_instance_type,\n",
    "        TrainingInstanceType=train_instance_type,\n",
    "        TrainingInstanceCount=train_instance_count,\n",
    "        ModelApprovalStatus=\"PendingManualApproval\",\n",
    "        InputDataUrl=input_s3_url\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57742436-a460-4ed8-a79d-131c63261b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment this call if you want the notebook to wait until the pipeline's execution finished\n",
    "# Execution time of this pipeline is about 8 minutes\n",
    "# execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573b6e4-d905-454e-9d14-6ea1af592279",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70ee2a-a099-467a-b13c-85fc567f1e4f",
   "metadata": {},
   "source": [
    "You can follow the pipeline execution in Studio by selecting **Pipelines** in **SageMaker resources** widget:\n",
    "\n",
    "<img src=\"img/pipelines-widget.png\" width=\"400\"/>\n",
    "\n",
    "After a successful execution you see the pipeline execution graph and can browse details for each pipeline step:\n",
    "\n",
    "![](img/pipeline-execution-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1eb4e6-6010-49c3-ada7-9935bf8644b8",
   "metadata": {},
   "source": [
    "## Continue with the step 4\n",
    "open the step 4 [notebook](04-sagemaker-project.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9dea1-ec92-4a28-8523-31c990c274a4",
   "metadata": {},
   "source": [
    "## Further development ideas for your real-world projects\n",
    "- Add model evaluation step and [conditional registering](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-condition) of a model in the model registry depending on the model performance\n",
    "- Add [data quality check](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-quality-check) and [model explainability](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-clarify-check) steps\n",
    "- Add event-driven launching of the ML pipeline as soon as a new dataset is uploaded to an Amazon S3 bucket. You can use [Amazon EventBridge integeration](https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html#pipeline-eventbridge-schedule) to implement various event-driven workflows\n",
    "- Use a designated IAM execution role for the pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f9efc-0920-4077-bd58-9f5514550412",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "- [Automate Machine Learning Workflows](https://aws.amazon.com/getting-started/hands-on/machine-learning-tutorial-mlops-automate-ml-workflows/)\n",
    "- [Amazon SageMaker Feature Store workshop](https://github.com/aws-samples/amazon-sagemaker-feature-store-end-to-end-workshop)\n",
    "- [Amazon SageMaker Model Building Pipeline](https://github.com/aws/sagemaker-python-sdk/blob/master/doc/amazon_sagemaker_model_building_pipeline.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874abcd0-2241-4118-9191-011215b861cd",
   "metadata": {},
   "source": [
    "# Shutdown kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bfea5-a835-4adc-81f0-e5355bf53a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc7baf-75d2-43fc-9c23-aadb29b72047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
